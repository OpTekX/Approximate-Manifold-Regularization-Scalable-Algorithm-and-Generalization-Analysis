\documentclass{article}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{bm}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb,amsfonts}
\usepackage{algorithm}
\usepackage{algorithmic}
\newcommand{\argmax}{\operatornamewithlimits{arg\,max}}
\newcommand{\argmin}{\operatornamewithlimits{arg\,min}}
\newtheorem{assumption}{Assumption}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\usepackage[small]{caption}

\title{Fast and Scalable Graph-Based Semi-Supervised Learning}

\begin{document}
\maketitle

\begin{abstract}
We proposed a novel semi-supervised learning framework combining manifold learning, random projection, iterative solvers and preconditioning.
Theoretical analysis shows our framework can achieve optimal statical accuracy with higher computation efficiency and less memory.
Comparing with direct methods, the time complexity is reduced from $O(n^3)$ to $O(n\sqrt{n})$ ,
and the memory complexity is reduced from $O(n^2)$ to $O(n)$.
Therefore, large scale Semi-Supervised Learning can be solved quickly on limited resource under our framework.
\end{abstract}

\section{Introduction}

\section{Notations and Preliminaries}
\subsection{Problem Definition}
Assume there is a fixed but unknown distribution $P$ on $\mathcal{X}\times\mathcal{Y}$,
where $\mathcal{X} = \mathbb{R}^d$ and $\mathcal{Y} = \mathbb{R}$.
Specifically, $l$ labeled examples $\{(\mathbf{x}_1,y_1), \cdots, (\mathbf{x}_l,y_l)\} \in \mathcal{X} \times \mathcal{Y}$ are drawn i.i.d from $P$
and $n-l$ unlabeled examples $\{\mathbf{x}_{l+1}, \cdots, \mathbf{x}_{n}\} \in \mathcal{X}$
are drawn i.i.d according to the marginal distribution $\mathcal{P}_X$ of $P$.

\subsection{Graph-Based Semi-Supervised Learning}
Manifold learning Methods based on Spectral Graph, known as Graph-Based SSL, is a typical solution to Semi-Supervised Learning
\cite{belkin2006manifold,chapelle2009semi,subramanya2014graph}.
Manifold learning is to find a smooth low-dimensional manifold embedded in the high-dimensional vector space, based on a set of sample points \cite{boothby1986introduction}.
And Spectral graph theory studies eigenvector and eigenvalues of matrices which is generated by graph \cite{chung1997spectral}.
To ensure the solution is smooth with respect to both the ambient space and the marginal distribution $\mathcal{P}_X$,
following manifold learning scheme is introduced in \cite{belkin2006manifold}
\begin{align*}
f^\ast=\argmin \limits_{f \in \mathcal{H}_K}
\frac{1}{l} \sum_{i=1}^l \ell(y_i, f(\mathbf{x}_i))
+\lambda_A\|f\|_K^2
+\lambda_I\|f\|_I^2
\end{align*}
where $K$ is a Mercer kernel $K:\mathcal{X} \times \mathcal{X} \to \mathbb{R}$ and there is an associated RKHS $\mathcal{H}_K$.
Intuitively, $\|f\|_I^2$ is an approximate penalty term that reflect the complexity of the function in \emph{intrinsic} geometry of $\mathcal{P}_X$,
while $\|f\|_K^2$ reflects the complexity of the function in the \emph{abient} space.

When Marginal $\mathcal{P}_X$ is unknown, the optimization problem becomes
\begin{align*}
f^\ast=\argmin \limits_{f \in \mathcal{H}_K}
\frac{1}{l} \sum_{i=1}^l \ell(y_i, f(\mathbf{x}_i))
+\lambda_A\|f\|_K^2
+\lambda_I \int_{\mathbf{x} \in \mathcal{M}} \|\nabla_\mathcal{M}f\|^2 d\mathcal{P}_X(\mathbf{x})
\end{align*}
where $\mathcal{M}$ is compact submanifold $\mathcal{M} \in \mathbb{R}^n$.

Based on different definitions of similarity between two points, there are two kinds of Graph-Based SSL: global methods and local methods.
Global methods calculate connections between all pair points. But local methods only consider connections between neighbourhood points.
In this paper, we apply the widely used local method called Laplacian Eigenmaps,
which is to find a map to keep local geometry features of points on average.
By choosing exponential weights of the adjacency graph, there is convergence of the graph Laplacian to the Laplace-Beltrami operator on the manifold
\begin{align}\label{mani_opt1}
f^\ast=\argmin \limits_{f \in \mathcal{H}_K}
\frac{1}{l} \sum_{i=1}^l \ell(y_i, f(\mathbf{x}_i))
+\lambda_A\|f\|_K^2
+\frac{\lambda_I }{n^2} \mathbf{f}^T \mathbf{L} \mathbf{f}
\end{align}
where $\mathbf{L}$ is the graph Laplacian matrix by $\mathbf{L}=\mathbf{D}-\mathbf{W}$
and $\mathbf{f}=[f(\mathbf{x}_1), \cdots, f(\mathbf{x}_{n})]^T$.
Here, $\mathbf{W} \in \mathbb{R}^{n \times n}$ records undirected weight between points
and the diagonal matrix $\mathbf{D}$ is given by $D_{ii}=\sum_{j=1}^n W_{ij}$.

\begin{theorem}
The Represent Theorem was given in \cite{belkin2006manifold}, which can be used to expansion of kernel functions in terms of the labeled and unlabeled data
\begin{align}\label{represent}
f^\ast(\mathbf{x})=\sum_{i=1}^n \alpha_i K(\mathbf{x}_i, \mathbf{x})
\end{align}
where $K$ is a positive definite kernel.
\end{theorem}

\subsection{Laplacian Regularized Least Squares}
When the loss function $\ell$ use the squared loss, the optimization problem in (\ref{mani_opt1}) becomes
\begin{align}\label{mani_opt2}
\argmin \limits_{f \in \mathcal{H}_K}
\frac{1}{l} \sum_{i=1}^l (y_i-f(\mathbf{x}_i))^2
+\lambda_A\|f\|_K^2
+\frac{\lambda_I }{n^2} \mathbf{f}^T \mathbf{L} \mathbf{f}
\end{align}
Applying the Represent Theorem, we arrive at a convex differentiable objective function of $\boldsymbol{\alpha}=[\alpha_1, \cdots, \alpha_n]^T$
\begin{align*}
\boldsymbol{\alpha}^\ast=\argmin \limits_{\boldsymbol{\alpha} \in \mathbb{R}^n}
\frac{1}{l} (\mathbf{Y}-\mathbf{J}\mathbf{K}\boldsymbol{\alpha})^T(-\mathbf{J}\mathbf{K}))
+\lambda_A \boldsymbol{\alpha}^T \mathbf{K} \boldsymbol{\alpha}
+\frac{\lambda_I l}{n^2} \boldsymbol{\alpha}^T \mathbf{K} \mathbf{L} \mathbf{K} \boldsymbol{\alpha}
\end{align*}
Setting the derivation of the objective function be zero leads following solution, which reduce computations to a linear system
\begin{align}\label{eq1}
(\mathbf{J}\mathbf{K}+\lambda_A l\mathbf{I}+\frac{\lambda_I l}{n^2}\mathbf{L} \mathbf{K})\boldsymbol{\alpha}^\ast=\mathbf{Y}
\end{align}

\emph{Computations.} For large datasets, it is a challenge to solve Equation (\ref{eq1}) in a direct approach,
which needs $O(n^2)$ in space and $O(n^2d+n^3)$ in time.

\subsection{Random Projection}
To accelerate large scale kernelized Semi-Supervised Learning,
we can use random subsampling to get a approximate matrix $\mathbf{K}_{ns}$
instead of primal kernel matrix $\mathbf{K}_{nn}$.
There are two kinds of popular random projection methods: Nystr\"om \cite{williams2001using,smola2000sparse}
and random features \cite{rahimi2008random}.
In this work, we focus on a basic Nystr\"om approach based on following form
\begin{align*}
    \tilde{f}^s_\lambda(\mathbf{x})=\sum_{i=1}^s \tilde{\alpha}_i K(\mathbf{x}_i, \mathbf{x}),
    \qquad
    \text{with} \quad \{\tilde{\mathbf{x}}_1, \cdots, \tilde{\mathbf{x}}_s\}
    \in \{\mathbf{x}_1, \cdots,\mathbf{x}_l, \mathbf{x}_{l+1}, \cdots, \mathbf{x}_n\}
\end{align*}
defined only $s$ train examples sampled uniformly from whole $n$ examples.
Then, only $s$ coefficients are needed in following linear system derived from (\ref{mani_opt2})
based on Nystr\"om approach
\begin{align*}
\tilde{f}^s_\lambda=\argmin \limits_{f \in \mathcal{H}_K}
\frac{1}{l} \sum_{i=1}^l (y_i-\mathbf{K}_{ls(i)}\boldsymbol{\alpha})^2
+\lambda_A \boldsymbol{\alpha}^T \mathbf{K}_{ss} \boldsymbol{\alpha}
+\frac{\lambda_I }{n^2} (\mathbf{K}_{ns}\boldsymbol{\alpha})^T\mathbf{L}(\mathbf{K}_{ns}\boldsymbol{\alpha})
\end{align*}
Let the derivation of the objective function be zero, we can get
\begin{align*}
\frac{2}{l}\mathbf{K}^T_{ls}(\mathbf{K}_{ls}\boldsymbol{\alpha}-\mathbf{y})
+2\lambda_A\mathbf{K}_{ss}\boldsymbol{\alpha}
+\frac{2\lambda_I}{n^2}\mathbf{K}_{ns}^T\mathbf{L}\mathbf{K}_{ns}\boldsymbol{\alpha}
=\mathbf{0}
\end{align*}
Then, we can get a linear system
\begin{align}\label{eq2}
(\mathbf{K}_{ls}^T\mathbf{K}_{ls}
+\lambda_A l \mathbf{K}_{ss}
+\frac{\lambda_I l}{n^2}\mathbf{K}_{ns}^T\mathbf{L}\mathbf{K}_{ns})
\boldsymbol{\alpha}=\mathbf{K}_{ls}^T\mathbf{y}
\end{align}

\emph{Computations.} Direct method solving equation (\ref{eq2}) needs $O(ns)$ to store $\mathbf{K}_{ns}$ in space
while it needs only $O(s^2)$ memory if $\mathbf{K}_{ls}^T\mathbf{K}_{ls}$
and $\mathbf{K}_{ns}^T\mathbf{L}\mathbf{K}_{ns}$ are computed in blocks of dimension at most $s \times s$,
and $O(ns^2)$ for computation.

\emph{Statistics.} As long as $s \gg n$, random projection can dramatically reduce memory cost.
However, a question arises of whether this comes at expenses of statistical accuracy
and of how many random examples is needed.
Recent work shows that $s=O(\sqrt{n})$ is enough to achieve statistical accuracy.
\subsection{Precondition and Gradient Methods}
The condition number is the radio between the largest and smallest singular value of matrix defined
in the problem \cite{saad2003iterative},
while it can capture the time complexity of iteratively solving the corresponding linear system.
For a basic iterative method to solve (\ref{eq2})
\begin{align}\label{basic_iter}
\boldsymbol{\alpha}_t=\boldsymbol{\alpha}_{t-1}
+\tau[(\mathbf{K}_{ls}^T\mathbf{K}_{ls}
+\lambda_A l \mathbf{K}_{ss}
+\frac{\lambda_I l}{n^2}\mathbf{K}_{ns}^T\mathbf{L}\mathbf{K}_{ns})
\boldsymbol{\alpha}-\mathbf{K}_{ls}^T\mathbf{y}]
\end{align}
By the above gradient decent method, the number of iterations needed for $\epsilon$ accurate solution of problem (\ref{eq2})
\begin{align*}
t=O(cond(\mathbf{K}_{ls}^T\mathbf{K}_{ls}
+\lambda_A l \mathbf{K}_{ss}
+\frac{\lambda_I l}{n^2}\mathbf{K}_{ns}^T\mathbf{L}\mathbf{K}_{ns})\log(1/\epsilon)).
\end{align*}
It is shown in \cite{camoriano2016nytro} that $t=O(\sqrt{n})$ can achieve good statistical properties.
However, preconditioning can accelerate iterative methods by using a suitable matrix $B$ to get better condition number. For (\ref{eq2}), we can get approximate matrix $\mathbf{B}$ by
\begin{align*}
\mathbf{B}\mathbf{B}^T \approx
(\mathbf{K}_{ls}^T\mathbf{K}_{ls}
+\lambda_A l \mathbf{K}_{ss}
+\frac{\lambda_I l}{n^2}\mathbf{K}_{ns}^T\mathbf{L}\mathbf{K}_{ns})^{-1}
=H^{-1}
\end{align*}
and $\mathbf{B}^T\mathbf{H}\mathbf{B}\boldsymbol{\beta}=\mathbf{B}^T\mathbf{y}$.
Clearly, $\boldsymbol{\alpha}_\ast=\mathbf{B}\boldsymbol{\beta}_\ast$ is solution of (\ref{eq2}).

We can use following form to approximate $H$ and $B$ is the Cholesky decomposition of $H^{-1}$,
and the approximate is based on random subsampling which can be Nystr\"om approximation.
\begin{align}\label{final_appr}
\boldsymbol{B}\boldsymbol{B}^T = 
(\frac{l}{l'}\mathbf{K}_{l's}^T\mathbf{K}_{l's}
+\lambda_A l \mathbf{K}_{ss}
+\frac{\lambda_I l(n+s)}{2ns^2}\mathbf{K}_{ss}^T\mathbf{L}_{ss}\mathbf{K}_{ss})^{-1}
\end{align}
where $l'$ is the number of subsampling examples from labeled data and $l' < s$.
The above preconditioning is a natural approximation of the ideal preconditioning of problem (\label{eq2}) that is 
$\boldsymbol{B}\boldsymbol{B}^T=H^{-1}$.
Specifically, of last part, $\mathbf{K}_{ns}$ becomes $\mathbf{K}_{ss}$ by subsamping uniformly from $n$ to $s$
which is partly from labeled data and partly from unlabeled data.

\emph{Computation.} Obviously, we can use (\ref{final_appr}) and
$\mathbf{B}^T\mathbf{H}\mathbf{B}\boldsymbol{\beta}=\mathbf{B}^T\mathbf{y}$
to solve linear system (\ref{eq2}) via conjugate gradient \cite{saad2003iterative} known as CG.
Thus, our method needs $O(nst+s^3)$ in time and $O(s^2)$ in space.

\section{Algorithms}
Our approach is combination of approximation, iterative method and preconditioning,
which consists two level approximation:
(1) Approximation on primal problem, resulting linear system (\ref{eq2}),
(2) Approximation on preconditioning, resulting preconditioner (\ref{final_appr}).
Finally, solving the problem by preconditioner and CG.
\begin{algorithm}\label{alg}
\caption{Fast and Scalable Semi-Supervised Learning}
\begin{algorithmic}[1]
\REQUIRE $l$ labeled examples $\{(\mathbf{x}_i, y_i)\}_{i=1}^l$, $n-l$ unlabeled examples $\{\mathbf{x}_j\}_{j=l+1}^n$.
Parameters: $\lambda_A, \lambda_I$, kernel method $K$ and subsampling size $s$.
\ENSURE coefficients $\boldsymbol{\alpha}$
\STATE Construct adjacent graph by k-NN or graph kernel with all $n$ nodes.
\STATE Define edge weight $W_{ij}=\exp ^{-\|\mathbf{x}_i-\mathbf{x}_j\|^2/4t}$ by heat kernel.
\STATE Compute graph Laplacian matrix $\mathbf{L}=\mathbf{D}-\mathbf{W}$
where $\mathbf{D}$ is a diagonal matrix given by $D_{ii}=\sum_{j=1}^n W_{ij}$.
\STATE Compute kernel matrix $\mathbf{K}_{ij}=K(\mathbf{x}_i, \mathbf{x}_j)$
\STATE Subsample uniformly on primal problem to get $\mathbf{K}_{ns}$ by Nystr\"om
\STATE Calculate matrix $
\mathbf{H}=\mathbf{K}_{ls}^T\mathbf{K}_{ls}
+\lambda_A l \mathbf{K}_{ss}
+\frac{\lambda_I l}{n^2}\mathbf{K}_{ns}^T\mathbf{L}\mathbf{K}_{ns}$
\STATE Compute preconditioner $\tilde{\mathbf{H}}=
(\frac{l}{l'}\mathbf{K}_{l's}^T\mathbf{K}_{l's}
+\lambda_A l \mathbf{K}_{ss}
+\frac{\lambda_I l(n+s)}{2ns^2}\mathbf{K}_{ss}^T\mathbf{L}_{ss}\mathbf{K}_{ss})$ 
via subsampling uniformly by Nystr\"om
\STATE Compute $\mathbf{B}\mathbf{B}^T=\tilde{\mathbf{H}}^{-1}$ with Cholesky decomposition of preconditioner by Nystr\"om
\STATE Solve $\mathbf{B}^T\mathbf{H}\mathbf{B}\boldsymbol{\beta}=\mathbf{B}^T\mathbf{y}$ via Conjugate Gradient.
\STATE Compute $\boldsymbol{\alpha}_\ast=\mathbf{B}\boldsymbol{\beta}_\ast$
\end{algorithmic}
\end{algorithm}

\section{Convergence and Complexity Analysis}
Firstly, statistical analysis about Nystr\"om method on Graph-based Semi-Supervised Learning
are given in \cite{rastogi2017manifold}. According to its \textbf{Theorem 3.1}, we can see the convergence rate is
$O(n^{-\frac{br}{2br+b+1}})$, which can easily get convergence rate at $O(\frac{1}{\sqrt{n}})$. Meanwhile, convergence about approximation on preconditioner is analysed in \cite{rudi2017falkon}, whose proof of \textbf{Theorem 3} shows approximation on preconditioner also can achieve optimal statistical properties.
Analysis in \cite{rastogi2017manifold, rudi2017falkon} shows at least $s=O(\sqrt{n})$ subsampling examples
and $t=O(\log n)$ iterations can give good theoretical guarantee.
Thus, our method needs only $O(n)$ space and $O(n\sqrt{n})$ time to get optimal statistical accuracy.

More details will be added in future.

\section{Empirical Study}

\bibliography{Bib_file}
\bibliographystyle{plain}
\end{document}
